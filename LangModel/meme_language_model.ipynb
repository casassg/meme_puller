{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import re, string, unicodedata\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words.split():\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word.replace('\\n', ' '))\n",
    "    return ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "everything = []\n",
    "data = json.load(open('Archive/ComedyCemetery.json'))\n",
    "data2 = json.load(open('Archive/dankmemes.json'))\n",
    "data3 = json.load(open('Archive/MemeEconomy.json'))\n",
    "data4 = json.load(open('Archive/memes.json'))\n",
    "data5 = json.load(open('Archive/wholesomememes.json'))\n",
    "\n",
    "for meme in data:\n",
    "    title = remove_emoji(meme[\"title\"])\n",
    "    everything.append(remove_non_ascii(title))\n",
    "\n",
    "for meme in data2:\n",
    "    title = remove_emoji(meme[\"title\"])\n",
    "    everything.append(remove_non_ascii(title))\n",
    "\n",
    "for meme in data3:\n",
    "    title = remove_emoji(meme[\"title\"])\n",
    "    everything.append(remove_non_ascii(title))\n",
    "\n",
    "for meme in data4:\n",
    "    title = remove_emoji(meme[\"title\"])\n",
    "    everything.append(remove_non_ascii(title))\n",
    "\n",
    "for meme in data5:\n",
    "    title = remove_emoji(meme[\"title\"])\n",
    "    everything.append(remove_non_ascii(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('meme_captions.txt', 'w') as f:\n",
    "    for e in everything:\n",
    "        f.write(e)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 193001\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "cleaning and preprocessing of the meme captions. \n",
    "prepend each with <s> and append with </s>. --> TO DO take this out?\n",
    "in separate file also working to just do padding of single captions rather than combining all with <s> and </s> delimiters\n",
    "^ I'll push this one to the repo later, have to finish it first :) it will probably work better though \n",
    "'''\n",
    "\n",
    "import string\n",
    "\n",
    "filename = 'meme_captions.txt'\n",
    "\n",
    "# load captions into memory\n",
    "lines = open(filename).read().splitlines()\n",
    "\n",
    "# tokenize and preprocess each caption\n",
    "all_tokens = []\n",
    "for caption in lines:\n",
    "    # split into tokens by white space\n",
    "    tokens = caption.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    tokens.insert(0, '<s>')\n",
    "    tokens.append('</s>')\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# organize into sequences of 20 tokens\n",
    "length = 20 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(all_tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = all_tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "\n",
    "    \n",
    "# save sequences to file\n",
    "out_filename = 'cleaned_captions.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('cleaned_captions.txt').read().splitlines()\n",
    "\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 50)            707900    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 14158)             1429958   \n",
      "=================================================================\n",
      "Total params: 2,288,758\n",
      "Trainable params: 2,288,758\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "193001/193001 [==============================] - 1729s 9ms/step - loss: 3.6162 - acc: 0.3000\n",
      "Epoch 2/50\n",
      "193001/193001 [==============================] - 1229s 6ms/step - loss: 3.5652 - acc: 0.3052\n",
      "Epoch 3/50\n",
      "193001/193001 [==============================] - 1292s 7ms/step - loss: 3.5108 - acc: 0.3112\n",
      "Epoch 4/50\n",
      "193001/193001 [==============================] - 1520s 8ms/step - loss: 3.5378 - acc: 0.3100\n",
      "Epoch 5/50\n",
      "193001/193001 [==============================] - 1289s 7ms/step - loss: 3.6040 - acc: 0.3063\n",
      "Epoch 6/50\n",
      "193001/193001 [==============================] - 1254s 6ms/step - loss: 3.6094 - acc: 0.3074\n",
      "Epoch 7/50\n",
      "193001/193001 [==============================] - 1325s 7ms/step - loss: 3.5144 - acc: 0.3143\n",
      "Epoch 8/50\n",
      " 71808/193001 [==========>...................] - ETA: 15:00 - loss: 3.4167 - acc: 0.3253"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('caption_model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('caption_tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "always wanted to contribute i hope this drawing i did is wholesome enough </s> <s> this is fantastic have some karma\n",
      "\n",
      "server s s i guess i was a good friend s s i love this thread s s i love\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# load the model\n",
    "model = load_model('caption_model.h5')\n",
    "tokenizer = load(open('caption_tokenizer.pkl', 'rb'))\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'cleaned_captions.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print('seed text: \\n')\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "n_words = 20\n",
    "\n",
    "result = list()\n",
    "in_text = seed_text\n",
    "# generate a fixed number of words\n",
    "for _ in range(n_words):\n",
    "    # encode the text as integer\n",
    "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "    # truncate sequences to a fixed length\n",
    "    encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "    # predict probabilities for each word\n",
    "    yhat = model.predict_classes(encoded, verbose=0)\n",
    "    # map predicted word index to word\n",
    "    out_word = ''\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == yhat:\n",
    "            out_word = word\n",
    "            break\n",
    "    # append to input\n",
    "    in_text += ' ' + out_word\n",
    "    result.append(out_word)\n",
    "    \n",
    "meme_caption = ' '.join(result)\n",
    "print('generated caption: \\n')\n",
    "print(meme_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
